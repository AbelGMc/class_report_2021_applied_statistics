{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b12c55-a39d-4002-8438-f99b2be5eccd",
   "metadata": {},
   "source": [
    "## Improvement\n",
    "\n",
    "There are three points where I can improve\n",
    "\n",
    "* Better GP fitting by tuning\n",
    "        * sigma2_init = np.max(np.abs(y))\\**2/4\n",
    "        * Fit failed. Print out a warning, and use the initial guesses for fit parameters. This only really seems to happen for objects where the lightcurve is almost entirely noise.\n",
    "* classification for extra/in galaxies seperately\n",
    "* solve unbalance by augumentation\n",
    "    * For class with number <300, 50X\n",
    "    * with number 300~1000, 20X\n",
    "    * with number >1000, 10X\n",
    "* More features\n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e13eb89-90fa-46d1-8d1f-aa374171a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         object_id         mjd  passband        flux   flux_err  detected\n",
      "0              615  59750.4229         2 -544.810303   3.622952         1\n",
      "1              615  59750.4306         1 -816.434326   5.553370         1\n",
      "2              615  59750.4383         3 -471.385529   3.801213         1\n",
      "3              615  59750.4450         4 -388.984985  11.395031         1\n",
      "4              615  59752.4070         2 -681.858887   4.041204         1\n",
      "...            ...         ...       ...         ...        ...       ...\n",
      "1421700  130779836  60555.9838         4  -39.881969  46.477093         0\n",
      "1421701  130779836  60560.0459         1   14.894439  18.947685         0\n",
      "1421702  130779836  60571.0225         5   30.593130  50.695290         0\n",
      "1421703  130779836  60585.9974         4  -23.471439  44.819859         0\n",
      "1421704  130779836  60588.0372         0  -41.214264  51.665123         0\n",
      "\n",
      "[1421705 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyreadr\n",
    "import copy\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stat\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.stats import biweight_location\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct,Matern,WhiteKernel\n",
    "project_data = pyreadr.read_r('../Data/project_data.Rdata')\n",
    "train_data = project_data[\"train_data\"]\n",
    "test_data = project_data[\"test_data\"]\n",
    "train_meta_data = project_data[\"train_meta_data\"]\n",
    "test_meta_data = project_data[\"test_meta_data\"]\n",
    "\n",
    "n_train = train_meta_data.shape[0]\n",
    "n_test = test_meta_data.shape[0]\n",
    "###------------------------------------------------------------------------------------------\n",
    "train_meta_data = train_meta_data.rename(train_meta_data[\"object_id\"])\n",
    "test_meta_data = test_meta_data.rename(test_meta_data[\"object_id\"])\n",
    "ob2id = {train_meta_data.iloc[i,0]:i  for i in range(n_train)}\n",
    "print(train_data)\n",
    "\n",
    "## u, g, r, i, z, Y = 0, 1, 2, 3, 4, 5 \n",
    "num2wl = {}  # number to wave length\n",
    "filters = [\"u\",\"g\",\"r\",\"i\",\"z\",\"y\"]  \n",
    "for i in range(6):\n",
    "    filter_data = np.loadtxt(\"../Data/filter_%s.dat.txt\"%(filters[i]))\n",
    "    num2wl[str(i)] = (np.sum(filter_data[:,0]*filter_data[:,1])/np.sum(filter_data[:,1]) )\n",
    "pb_wavelength = np.array(list(num2wl.values()))\n",
    "\n",
    "            \n",
    "loaded_data = np.load(\"../Data/3_curve.npz\",allow_pickle=True)\n",
    "curve_data = loaded_data[\"curve_data\"]\n",
    "\n",
    "train_data = loaded_data[\"train_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aeb358-31e3-4bbe-bf6a-47e44b798378",
   "metadata": {},
   "source": [
    "## 1. Better GP fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500c82ae-3e9d-4fb2-80cb-e3205b461895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Initial length scale for time is changed from 1 to 20\n",
    "\n",
    "# warnings.filterwarnings(\"error\")\n",
    "\n",
    "# gp_regressor_list = []\n",
    "# warning_list = []\n",
    "# t_s = time.time()\n",
    "# for i in range(n_train):    \n",
    "#     X = curve_data[i][curve_data[i][:,2]==1][:,[1,6]]\n",
    "#     y = curve_data[i][curve_data[i][:,2]==1,3]\n",
    "    \n",
    "#     sigma2_init = np.max(np.abs(y))**2/4\n",
    "\n",
    "#     noise2_upper = min(np.median(np.abs(y))**2,100,sigma2_init/4)\n",
    "#     noise2_lower = min(1e-8,noise2_upper/2)\n",
    "\n",
    "#     kernel = sigma2_init * Matern(length_scale=[20.0,600],length_scale_bounds=[(1e-5,10000),(600,600)], nu=1.5) + WhiteKernel(noise_level=0.1,noise_level_bounds=(noise2_lower,noise2_upper))\n",
    "    \n",
    "#     try:\n",
    "#         gpr = GaussianProcessRegressor(kernel=kernel,random_state=0,n_restarts_optimizer=20).fit(X,y)\n",
    "#     except Warning:\n",
    "#         print(\"convergence warning at curve data %d.\"%i)\n",
    "#         warning_list.append(i)\n",
    "#     gp_regressor_list.append(gpr)\n",
    "#     for k in range(10):\n",
    "#         if(i==(k+1)*int(n_train/10)):\n",
    "#             print(\"%d%% done...\"%(10*(k+1)))\n",
    "# t_e = time.time()\n",
    "# print(t_e-t_s)\n",
    "           \n",
    "# warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859f102-c92d-4876-8aa4-cbfcc6b7c4f4",
   "metadata": {},
   "source": [
    "```\n",
    "convergence warning at curve data 36.\n",
    "convergence warning at curve data 300.\n",
    "convergence warning at curve data 366.\n",
    "convergence warning at curve data 412.\n",
    "10% done...\n",
    "convergence warning at curve data 864.\n",
    "20% done...\n",
    "convergence warning at curve data 2266.\n",
    "30% done...\n",
    "40% done...\n",
    "convergence warning at curve data 3390.\n",
    "50% done...\n",
    "60% done...\n",
    "70% done...\n",
    "convergence warning at curve data 5552.\n",
    "80% done...\n",
    "90% done...\n",
    "100% done...\n",
    "2289.269389152527\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ea3dd8-1d33-4c5c-896b-a97426f4eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7848\n",
      "7848\n"
     ]
    }
   ],
   "source": [
    "# large_data_file = \"/Users/cjh/Desktop/Large_files/Code_data/class_applied_stat_LSST\"\n",
    "# with open(\"%s/5_gp_regressor.txt\"%large_data_file, \"wb\") as fp:   #Pickling dumping\n",
    "#      pickle.dump([gp_regressor_list,warning_list] ,fp)\n",
    "        \n",
    "large_data_file = \"/Users/cjh/Desktop/Large_files/Code_data/class_applied_stat_LSST\"\n",
    "with open(\"%s/gp_regressor_list_restrict_error.txt\"%large_data_file, \"rb\") as fp:   #Pickling loading\n",
    "    [gp_regressor_list,warning_list] = pickle.load(fp)\n",
    "print(len(gp_regressor_list))\n",
    "print(n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bcc0b-1d31-4890-bf6c-8840f6815ca1",
   "metadata": {},
   "source": [
    "## 2. data argumentation and GP feature extraction\n",
    "\n",
    "* Features from 1st:\n",
    "    * `host_photoz`\n",
    "    * `host_photoz_err`\n",
    "    * `[max,min]_flux_ratio _[blue,red]`: Normalized difference of the light curve colors at maximum/minimum light. Blue for g-i and red for i-y. Taking the difference of the fluxes in the two bands divided by their sum.\n",
    "    * `max_mag`: Peak magnitude of the GP flux prediction in the LSST i band.\n",
    "    * `frac_background`: Fraction of observations that have an absolute signal-to-noise less than 3.\n",
    "    * `time_[fwd,bwd]_max _[0.2,0.5]`:  the time in days for the light curve to rise (bwd) or decline (fwd) to a given fraction (either 20% or 50%) of maximum light in the LSST i band.\n",
    "    * `time_width_s2n_5`: Time difference in days between the first observation with a signal-to-noise greater than 5 and the last 1such observation (in any band).\n",
    "    * `count_max_rise _[20,50,100]`: Number of observations in any band between 20, 50, or 100 days before maximum light and 5 days after maximum light. (?)\n",
    "    * `total_s2n`: Total signal-to-noise of all observations of the object.\n",
    "    * `pos_flux_ratio`: Ratio of the maximum positive flux to the maximum-minus-minimum flux in the LSST i band.\n",
    "    * `max_dt`: Difference of the time of maximum in the LSST y and g bands in days\n",
    "    * `count_max_fall _[20,50,100]`: Number of observations in any band between 5 days before maximum light and 20, 50, or 100 days after maximum light.\n",
    "    * `percentile_diff _[10,30,70,90]_50`: Measurements of the distributions of the observed fluxes. \n",
    "    * `偏度峰度`：代替上面的quantile\n",
    "    * `time_[fwd,bwd]_max _[0.2,0.5]_ratio _[blue,red]`:Ratio of the rise/decline times calculated as described above in different bands.\n",
    "    * `[positive,negative] _width`: An estimate of the light curve “width”, the integral of the positive/negative parts of the GP flux predictions divided by the positive/negative maximum fluxes.\n",
    "    * `frac_s2n_[5,−5]`: Fraction of observations that have a signal greater than 5/less than −5 times the noise level.\n",
    "    * `length_scale`\n",
    "    * `peak_frac_2`: Ratio of the maximum flux in the second-most prominent peak in the light curve to the maximum flux in the main peak, averaged over all LSST bands. \n",
    "\n",
    "* features from the 4th\n",
    "    * Ratio features: I guess most of you did this. Instead of using raw features from each passband, I used their ratio over all passbands\n",
    "    * Hostgal_specz model: I trained a model to predict hostgal specz using training set+ test set with hostgal_specz. Then used this model's predictions as a feature.\n",
    "    * Using normal values and log transformed values together on Neural Net: Having both gives you the opportunity to do all four operations between the features (+, -, / *) because you can write log(xy) as log(x) + log(y).\n",
    "    * `Log Ensemble` \n",
    "    \n",
    "* Features in my code\n",
    "\n",
    "```\n",
    "\n",
    "colnames = {\n",
    "\n",
    "0:\"ra\",1:\"decl\",2:\"host_photoz\",3:\"host_photoz_err\",\n",
    "\n",
    "4:\"max_flux_ratio_blue\",5:\"min_flux_ratio _blue\",6:\"max_flux_ratio_red\",7:\"min_flux_ratio_red\",\n",
    "\n",
    "8:\"max_mag\",\n",
    "\n",
    "9:\"time_bwd_max_0.2\",10:\"time_bwd_max_0.5\",\n",
    "\n",
    "11:\"pos_flux_ratio\",12:\"max_dt\",\n",
    "\n",
    "13:\"skewness\",14:\"Kurtosis\",\n",
    "\n",
    "15:\"skewness_g_i\",16:\"Kurtosis_g_i\",17:\"skewness_i_y\",18:\"Kurtosis_i_y\",\n",
    "\n",
    "19:\"length_param\"\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d176c784-5e23-41b2-a183-5ba4a2a8701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = train_meta_data['target'].value_counts()\n",
    "class_to_numbers = {}\n",
    "for i in range(class_counts.shape[0]):\n",
    "    class_to_numbers[class_counts.index[i]] = np.floor(400/np.sqrt(class_counts.values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426a3783-6601-46ec-a168-666f71770787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.409531090723751\n",
      "{90: 8.0, 42: 11.0, 65: 12.0, 16: 13.0, 15: 17.0, 62: 18.0, 88: 20.0, 92: 25.0, 67: 27.0, 52: 29.0, 95: 30.0, 6: 32.0, 64: 39.0, 53: 73.0}\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.floor(400/np.sqrt(class_counts.values))*class_counts.values)/np.sum(class_counts.values))\n",
    "print(class_to_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0514ff-9fdf-442a-b0bf-e4d18adf701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To run\n",
    "### To run\n",
    "### To run\n",
    "### To run\n",
    "import functions_LSST_final as LSST\n",
    "\n",
    "n_added_timepoints = 200\n",
    "argumented_curves = LSST.get_argumented_curves(curve_data,gp_regressor_list,n_added_timepoints,class_to_numbers,train_meta_data,pb_wavelength)\n",
    "\n",
    "\n",
    "large_data_file = \"/Users/cjh/Desktop/Large_files/Code_data/class_applied_stat_LSST\"\n",
    "with open(\"%s/5_argumented_curves.txt\"%large_data_file, \"wb\") as fp:   #Pickling dumping\n",
    "     pickle.dump(argumented_curves ,fp)\n",
    "        \n",
    "# with open(\"%s/5_argumented_curves.txt\"%large_data_file, \"rb\") as fp:   #Pickling loading\n",
    "#     argumented_curves = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324606d-4dae-47b1-998c-6048047a7694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23159cec-8026-41f3-986a-ab89346049bd",
   "metadata": {},
   "source": [
    "## 3. Tuning two RF for extragalactic and galactic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d512c064-0b12-49bd-9811-78ffcec77798",
   "metadata": {},
   "outputs": [],
   "source": [
    "extragalactic = [15,42,52,62,64,67,88,90,95]\n",
    "colnames = {0:\"ra\",1:\"decl\",2:\"host_photoz\",3:\"host_photoz_err\",\n",
    "        4:\"max_flux_ratio_blue\",5:\"min_flux_ratio _blue\",6:\"max_flux_ratio_red\",7:\"min_flux_ratio_red\",\n",
    "        8:\"max_mag\",\n",
    "        9:\"time_bwd_max_0.2\",10:\"time_bwd_max_0.5\",\n",
    "        11:\"pos_flux_ratio\",12:\"max_dt\",\n",
    "        13:\"skewness\",14:\"Kurtosis\",\n",
    "        15:\"length_param\",\n",
    "        16:\"skewness_g_i\",17:\"Kurtosis_g_i\",18:\"skewness_i_y\",19:\"Kurtosis_i_y\",\n",
    "       }\n",
    "feature_names = [colnames[i] for i in range(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fcc71e-feec-46e8-831f-56800a4f7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X,y] = LSST.curves_to_training(argumented_curves[0],argumented_curves[1],pb_wavelength)\n",
    "index_extragalactic = [(class_name in extragalactic) for class_name in y]\n",
    "index_galactic = [(class_name not in extragalactic) for class_name in y]\n",
    "X_extragalactic,y_extragalactic = X[index_extragalactic,],y[index_extragalactic]\n",
    "X_galactic,y_galactic = X[index_galactic,],y[index_galactic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb1cf04-294f-4ffe-aaa8-3ca55d105f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_extragalactic = RandomForestClassifier(max_depth=50, random_state=0,oob_score=True)\n",
    "clf_extragalactic.fit(X_extragalactic, y_extragalactic)\n",
    "clf_extragalactic.score(X_extragalactic, y_extragalactic)  # 1.0\n",
    "\n",
    "print(clf_extragalactic.feature_importances_)\n",
    "print(clf_extragalactic.score(X_extragalactic,y_extragalactic))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(X_extragalactic.shape[1]), clf_extragalactic.feature_importances_,\n",
    "       color=\"r\")\n",
    "# If you want to define your own labels,\n",
    "# change indices to a list of labels on the following line.\n",
    "plt.yticks(range(X_extragalactic.shape[1]), feature_names)\n",
    "plt.ylim([-1, X_extragalactic.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c78c74-26bb-400a-b968-3a4e90ded060",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_galactic = RandomForestClassifier(max_depth=50, random_state=0,oob_score=True)\n",
    "clf_galactic.fit(X_galactic, y_galactic)\n",
    "clf_galactic.score(X_galactic, y_galactic)  # 1.0\n",
    "\n",
    "print(clf_extragalactic.feature_importances_)\n",
    "print(clf_galactic.score(X_galactic,y_galactic))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(X_galactic.shape[1]), clf_galactic.feature_importances_,\n",
    "       color=\"r\")\n",
    "# If you want to define your own labels,\n",
    "# change indices to a list of labels on the following line.\n",
    "plt.yticks(range(X_galactic.shape[1]), feature_names)\n",
    "plt.ylim([-1, X_galactic.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a73a8-b0d7-41cc-9026-b99b1c7a241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)\n",
    "index_extragalactic = [(class_name in extragalactic) for class_name in y_train]\n",
    "index_galactic = [(class_name not in extragalactic) for class_name in y_train]\n",
    "\n",
    "X_extragalactic,y_extragalactic = X_train[index_extragalactic,],y_train[index_extragalactic]\n",
    "X_galactic,y_galactic = X_train[index_galactic,],y_train[index_galactic]\n",
    "\n",
    "clf_extragalactic = RandomForestClassifier(max_depth=50, random_state=0,oob_score=True)\n",
    "clf_extragalactic.fit(X_extragalactic, y_extragalactic)\n",
    "\n",
    "clf_galactic = RandomForestClassifier(max_depth=50, random_state=0,oob_score=True)\n",
    "clf_galactic.fit(X_galactic, y_galactic)\n",
    "\n",
    "######------------------------\n",
    "\n",
    "index_extragalactic_test = [(class_name in extragalactic) for class_name in y_test]\n",
    "index_galactic_test = [(class_name not in extragalactic) for class_name in y_test]\n",
    "X_extragalactic_test,y_extragalactic_test = X_test[index_extragalactic,],y_test[index_extragalactic]\n",
    "X_galactic_test,y_galactic_test = X_test[index_galactic,],y_test[index_galactic]\n",
    "\n",
    "total_score = clf_extragalactic.score(X_extragalactic_test,y_extragalactic_test)*np.sum(index_extragalactic_test) + clf_galactic.score(X_galactic_test,y_galactic_test)*np.sum(index_galactic_test)\n",
    "total_score = total_score/(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cf310-e39b-4628-ae0b-01834e62b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_confusion_matrix(estimator=clf, X=np.concatenate(X_extragalactic_test,X_galactic_test), y_true=np.concatenate(y_extragalactic_test,y_galactic_test),\n",
    "                                 cmap=plt.cm.Blues,normalize=\"true\")\n",
    "plt.xticks( rotation='vertical')\n",
    "plt.savefig(\"../Images/new_confusion_arg_arg.png\",bbox_inches = \"tight\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae2a63-3127-406a-b242-dcb063e717aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8b76e-1b77-4c66-bfaa-ef4cc8c71fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_added_timepoints = 200\n",
    "[raw_curves, raw_meta] = LSST.get_curves_from_raw(curve_data,gp_regressor_list,n_added_timepoints,train_meta_data,pb_wavelength)\n",
    "[raw_X,raw_y] = LSST.curves_to_training(raw_curves,raw_meta,pb_wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf58b71-cf78-432b-8dd1-893bb1e3bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "(....).score(raw_X, raw_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
